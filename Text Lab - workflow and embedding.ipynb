{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "pycharm": {}
   },
   "source": [
    "# Import libraries\n",
    "\n",
    "To allow easier navigation please go to\n",
    "\n",
    "[edit]$\\rightarrow$[nbextensions config]\n",
    "\n",
    "This will open a new window. In this new window select/tick\n",
    "\n",
    "* Collapsible Headings\n",
    "* Table of Contents (2)\n",
    "\n",
    "You can then simple close that window, and refresh this page.\n",
    "\n",
    "In the toolbar above you can now click the following icon to get a contents menu.\n",
    "\n",
    "<img src=\"Images/ContentsButton.png\" align='middle'>\n",
    "\n",
    "Please now run the following code to make the notebook just a little more beautiful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "css_file_path = 'custom.css'\n",
    "styles = open(css_file_path, \"r\").read()\n",
    "HTML(styles) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "pycharm": {}
   },
   "source": [
    "We now install and then import the following libraries that you will use in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "!pip install gensim matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "\n",
    "import sklearn.feature_extraction.text as sklearntext\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Import the IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "We will be investigating data from the IMDB wedsite. The dataset is ready to be inported, but it can also be found [here](https://www.kaggle.com/PromptCloudHQ/imdb-data).\n",
    "\n",
    "Begin by loading the dataset into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "data_path = 'IMDB-Movie-Data.csv'\n",
    "df_all = pd.read_csv(data_path, sep=',')\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "We will be trying to predict if a movie is a drama given its description.\n",
    "\n",
    "There is a lot of extra information in the dataframe, `df_all`, so we'll begin by just extracting the desctiption and the genre.\n",
    "\n",
    "We call the new dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df = df_all[['Genre', 'Description']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "The only standardisation of the text we will do is to make the text lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df['Description'] = df['Description'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "It will be useful later on to have the length (number of words) of each movie description, so lets add that as a column to our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "To get the word counts we could use scikit-learn's [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), but we will introduce this tool in a later section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df['NumWords'] = df['Description'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "We will also need a column indicating if the genre is *drama* or *non-drama* (this will be our labels, y). We will have\n",
    "* 0 $\\rightarrow$ non-drama\n",
    "* 1 $\\rightarrow$ drama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df['Drama'] = df['Genre'].str.contains('Drama').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Our dataframe now has the form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Explorative data analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "One simple way of thinking about EDA is by asking which problems might *might* we run into with this dataset? For each possible problem we need to understand what the data looks like so we can deal with is. So lets look into\n",
    "  * Is there a wide variance in description length, or examples with no description at all?\n",
    "  * Is our data skewed (that is, do we have a strong imbalance between dramas and non-dramas)?\n",
    "  * The total number of words, and how many words occur only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Description lenghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Lets begin with the description lenths. We want to check the mean and std of the number of words in theinput texts, lets first plot these, and then we will use pandas to calculate the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df.hist(column='NumWords', by='Drama', layout=(1,2), bins=np.linspace(0, 80, 81), figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df.groupby('Drama').agg(['mean', 'median', 'sum', 'std'])['NumWords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Do these distributions look OK to work with?\n",
    "\n",
    "Does it matter that the mean isn't identical between the two categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Skewed data set\n",
    "\n",
    "Pandas' `.groupby` can also allow us very easily to see if we have a skewed dataset. By skewed we mean that the number of examples for one category is much less than for other other. Below we find this is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df.groupby('Drama').agg(['count'])['NumWords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Finally we actually want two `np.array` which store the inputs and labels for out ML model. These can now be easily taken from our dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.loc[:,'Description'])\n",
    "y = np.array(df.loc[:,'Drama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "To be clear, we now have two numpy arrays, one, `X` with a list of movie descriptions, and one, `y`, with labes 0 and 1 for if that movie is a drama or not. Lets have a look at a \"random\" sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{X[42]} -> {y[42]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Word counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "What about the total **number of words**, and the total **number of unique words**? To do this we will use scikit-learn's [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), which does a bag or words embedding. This is a bit to learn, but it will be useful later, so lets take a brief tour through the `CountVectorizer()` object, and it's output, `X_sparse`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "This sparse matrix `X_sparse` *is* a bag of words embedding, but for now we are using it as a convenient way of counting the number of words and unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Lets make a `CountVectorizer`, called `count_vect`, fit it to our input, `X`, and then transform our input texts, `X`, into a bag of words embedding called `X_sparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(lowercase=True)\n",
    "count_vect = count_vect.fit(X)\n",
    "X_sparse = count_vect.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "The bag of words embedding uses a python dictionary (like a hashmap) to convert words to numbers. In python, dictionaries work in one direction, so lets also make a dictionary to convert our numbers back to words (an 'inverse hashmap'). Note that in python accessing values in a dictionary is done like:\n",
    "\n",
    "```python\n",
    "value = dictionay[key]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "vocab = count_vect.vocabulary_\n",
    "vocab_inverse = {}\n",
    "for word, int_embedding in vocab.items(): # In Python 2: .iteritems()\n",
    "    vocab_inverse[int_embedding] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "We now have two dictionaries for our embeddings (these are what scikit-learn us using to convert between `X` and `X_sparse`):\n",
    "\n",
    "`vocab`: $\\:\\:\\textrm{word} \\rightarrow \\textrm{number}$\n",
    "\n",
    "`vocab_inverse`: $\\:\\:\\textrm{number} \\rightarrow \\textrm{word}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "If you want to, print out either of `vocab` or `vocab_inverse` to see what these dictionaries look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(vocab)\n",
    "print(vocab_inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "By investigating an example we should see what is contained within `X` and `X_sparse`. Note that `X` is a list of *all* the input data, so `X[0]` should be the description of the 'first' movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "`X_sparse` is a sparse matrix with all the input data encoded using bag of words. Again `X_sparse[42]` is the encoding of the 'first' movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(X_sparse[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Finally we can check the encodings of any words. For example number 5288 should be a word appearing twice in the description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(vocab_inverse[5288])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "In movie `X[20]` they mention a geologist. Which number word is `geologist` encoded as?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Back to the word count problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "OK, back to the original problem, counting the number of words and unique words. Now that we have our sparse matrix `X_sparse` this should be very easy. We can just use the `.sum()` method of sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "total_words = X_sparse.sum()\n",
    "total_words_unique = len(count_vect.vocabulary_)\n",
    "\n",
    "print('total_words: {}'.format(total_words))\n",
    "print('total_words_unique: {}'.format(total_words_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Why do we care about the total number of words? And why did we check the number of unique words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Splitting data into test and training sets\n",
    "\n",
    "We will just use scikit-learns built in function for this. Notice that `sklearn.model_selection.train_test_split` shuffles the dataset, and then splits it. This means that `X[0]` and `X_train[0]` (or `X_test[0]`) most likely won't represent the same movie description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Creating the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "In this section we are going to create 6 matrices, which we will use as input to our machine learning models in the next section. The 6 matrices will be the following:\n",
    "\n",
    "* `X_train_bow` and  `X_test_bow`\n",
    "* `X_train_tfidf` and `X_test_tfidf`\n",
    "* `X_train_wtv` and `X_test_wtv`\n",
    "\n",
    "That is a training and a test matrix for each of the three embeddings we are using: bag of words (bow), tf-idf (tfidf) and Word2Vec (wtv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Bag of Words (bow) and Text-frequency Inverse-document-frequency (tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "We have actually already created a bow emdedding using scikit-learn's `CountVectorizer`, however we need to now make this for the train and test sets. We will also use scikit-learn's [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for the tfidf embedding.\n",
    "\n",
    "Previously we used the `.fit()` and `.transform()`. Here we will take a closer look at what they are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "It is also common to see the `.fit_transform()` method which does boths steps at once, although it is a little less clear what is happening in that case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "First we essentially initialise a `CountVectorizer()` object, this is the tool we use to make our embedding. This object doesn't know about our dataset yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Then we need to 'fit' that vectorizer to our dataset (using the `.fit()` method). You can think of this as the vectorizer learning which set of words it need to be *able* to encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "count_vect = count_vect.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Now that we have a vectorizer (called `count_vect` in this case), we can use that to transform the dataset (i.e. do the bag of words embedding). We do this with the `.transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "X_train_bow = count_vect.transform(X_train)\n",
    "X_test_bow = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Now we will go through the process again, but instead using a tf-idf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(lowercase=True, max_df=1.0)\n",
    "tfidf_vect = tfidf_vect.fit(X)\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "You should print out some sample encodings from `X_train_bow` and `X_train_tfidf` to check you understand what these represent. You can also print the corresponding sample from `X_train`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "`X_train_bow` and `X_train_tfidf` are just scikit-learn *sparse matrices*, the same as `X_sparse` was, the methods used in the word counts section should work here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "print(X_train_bow[42])\n",
    "print(X_train_tfidf[42])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Dense (Word2Vec) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Before we do the actual word embeddings we will investigate how these embeddings work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Creating dense word embedding using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Using [gensim](https://radimrehurek.com/gensim/) one can create their own word2vec word embeddings.\n",
    "\n",
    "We will start by **building** a simple word2vec model following [this](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/) blog post. Here we train on the senteces given in the list `sentences`. This is obviously a *very* small 'dataset', and thus the results just show what an embedding can look like.\n",
    "\n",
    "If you wish, you can change either `min_count` or `size` to see what they do. But be sure to change them back to `size=100, min_count=1` before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Input dataset:\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "s1 = \"President Trump confirms CIA chief secretly visited North Korean leader last week\".split(\" \")\n",
    "s2 = \"Barbara Bush former US first lady literacy campaigner has died\".split(\" \")\n",
    "s3 = \"One person killed after US passenger jet with engine failure made emergency landing in Philadelphia\".split(\" \")\n",
    "s4 = \"The prime minister personally apologized Caribbean leaders\".split(\" \")\n",
    "\n",
    "sentences = [s1, s2, s3, s4]\n",
    "\n",
    "\n",
    "# Train the model:\n",
    "super_simple_model = Word2Vec(sentences,  size=100, min_count=1)\n",
    "\n",
    "print(super_simple_model)\n",
    "# print(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "After training the model we reduce the word embedding dimension from 100 -> 2 using [t-distributed Stochastic Neighbor Embedding](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) then plot the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Transform the data via PCA to 2D so we can visually plot it.\n",
    "model_embeddings = super_simple_model.wv[super_simple_model.wv.vocab]\n",
    "pca = TSNE(n_components=2)\n",
    "result = pca.fit_transform(model_embeddings)\n",
    "\n",
    "plt.figure(1, figsize=(16, 8))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(super_simple_model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "With so few words in our dataset, and since there is no clear meaning on the axes it's hard to assess the embedding, but the words have some strucutre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Exploring dense word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For english there are many large and high quality embeddings that already exist (pre built embeddings also exist for many other languages).\n",
    "\n",
    "We will import a word2vec embedding created from a dataset of roughly 100 billion words from google news. The embedding covers 3 million words, but to save space, we have cut it down to 200,000 words. Each word is encoded to a 300 dimensional vector, hence the binary file imported above is aroung 250 MB.\n",
    "\n",
    "Please run the next code section. If the download takes a while just move on to the step, we won't need the file for a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###!wget -O gW2V.bin https://www.dropbox.com/s/tu5045ajmj20ih9/googleNewsCut.bin?dl=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Lets now load the google news dataset into a gensim model object (this will take ~2 mins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('gW2V.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Gensim's `model` object has a range of useful methods for investigating the word2vec model. The rest of this section (down to **Word2Vec embedding for our IMDB data**) is now left completely open for you to play with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Lets see what an embedding looks like. You can print the whole thing if you like (just remove `[:20]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "word = 'house'\n",
    "print('Word: {}'.format(word))\n",
    "print('First 20 values of embedding:\\n{}'.format(model[word][:20]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "A classic example to demonstrate the usefulness of word2vec, is solving\n",
    "\n",
    "$X - woman = king - man$\n",
    "\n",
    "for $X$. Note that you can think of this as asking the question \"what is to woman, what king is to man?\"\n",
    "\n",
    "If you want a challenge use the fact that London is the capital of England to find the capital of Norway. What other capitals does it know? Does it know capitals at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3))\n",
    "\n",
    "print(model.most_similar(positive=['France', 'Viking'], negative=['Norway'], topn=3))\n",
    "\n",
    "print(model.most_similar(positive=['Tennis', 'Ronaldo'], negative=['Soccer'], topn=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "The method `.doesnt_match()` finds the word which is furtherest away from the other words in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# .split() simply splits the sentence into a list of words\n",
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Word embeddings are built by analysing which words are used in conjunction with other words in the data set. Thus biases within the dataset will be present in the embeddings.\n",
    "\n",
    "`.similarity()` calculate how silimar two words are using the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(model.similarity('man', 'nurse') - model.similarity('man', 'doctor'))\n",
    "print(model.similarity('women', 'nurse') - model.similarity('women', 'doctor'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Word2Vec embedding for our IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "We will just use the google news embeddings to change our movie description to word2vec vectors. It will be useful to [wrap the embeddings into a class](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/) with fit and transform methods such that we can use them easily with scikit-learn. Although we will not need to fit this, because the embeddings are already 'fitted'.\n",
    "\n",
    "We will use the simplest approach to encoding the description, which is to simply take the mean of the embeddings of all the words in the movie description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class WordVecVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in texts.split() if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for texts in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "wtv_vect = WordVecVectorizer(model)\n",
    "X_train_wtv = wtv_vect.transform(X_train)\n",
    "X_test_wtv = wtv_vect.transform(X_test)\n",
    "\n",
    "print(X_train_wtv.shape) # Should be 700 (num of example) by 300 (dim. of embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Fantastic, now we have both test and training data sets with bow, tf-idf and word2vec encoding. These are stored in the matrices\n",
    "\n",
    "* `X_train_bow` and  `X_test_bow`\n",
    "* `X_train_tfidf` and `X_test_tfidf`\n",
    "* `X_train_wtv` and `X_test_wtv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Training and predicting\n",
    "\n",
    "We will now try linear regression, SVM, a decision tree, and a random forest to flassify the movie genres for the three embeddings.\n",
    "\n",
    "The basic method is\n",
    " 1. Create a classifier object like `clf = LogisticRegression()`\n",
    " 2. Then fit this object to the training set using `.fit()`\n",
    " 3. Use the model to predict the labels for the test set using `.predict()`\n",
    " 4. Elavuate the model using one of the metris from [`sklearn.metrics`](http://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "First lets load the models from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "OK, step 1, create the classifier object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "clf_lg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Now fit this classifier. We will jsut use the tf-idf encodings, and of course we have to give the labels to it also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "clf_lg.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "And then evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "predictions = clf_lg.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('LR accuracy with tfidf: {}\\n'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "To understand the output, lets just look at a single example. Note that `clf_lr` is now our *fitted* linear regression model. Reading each line you can hopefully find what the `.predict()` and `.predict_proba` methods do. If not you can ask, or [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) is the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "i = 42\n",
    "print(X_train[i])\n",
    "print(clf_lg.predict(X_train_tfidf[i]))\n",
    "print(clf_lg.predict_proba(X_train_tfidf[i]))\n",
    "print(y_train[i]) # 0=non Drama, 1=Drama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Lets now find the accuracy of a *support vector machine* with tf-idf encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# The default SGDClassifier implements a SVM:\n",
    "clf_svm = SGDClassifier(random_state=42)\n",
    "clf_svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "predictions = clf_svm.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('SVM accuracy with tfidf: {}\\n'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Below we have named the models, as this will be used later. By setting the `random_state` variable we simply give the see to the random number generator a fixed value. You are free to play with this, or the number or trees, `n_estimators` in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "clf_dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "### Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "### Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "The following simply runs over models and encoding and looks at accuracy to judge which model is the best. We have not fine tuned the models at all, so this just provides an overview of what we have done and indicates which methods are most promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "ecodings = [X_train_bow, X_train_tfidf, X_train_wtv]\n",
    "ecodings_t = [X_test_bow, X_test_tfidf, X_test_wtv]\n",
    "models = [clf_lg, clf_svm, clf_rf, clf_dt]\n",
    "\n",
    "I = pd.Index([\"Bow\", \"Tf-idf\", \"Word2Vec\"], name=\"Encodings:\")\n",
    "C = pd.Index([\"LR\", \"SVM\", \"RF\", 'DT'], name=\"Models:\")\n",
    "df = pd.DataFrame(index=I, columns=C)\n",
    "for i, (e, e_t) in enumerate(zip(ecodings, ecodings_t)):\n",
    "    for j, m in enumerate(models):\n",
    "        m.fit(e, y_train)\n",
    "        predictions = m.predict(e_t)\n",
    "        df.iloc[i, j] = accuracy_score(y_test, predictions)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Adding  n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Thus far we in all of our word encodings we have only dealt with single words and have not taken into account the order of the words in the sentences. This can be done by using n-grams (sets of n words together which we treat as a single object).\n",
    "\n",
    "Although it is conceptually possible to create n-grams for the word2vec model this is not generally done and we will just investigate n-grams for the bag of words and tf-idf models.\n",
    "\n",
    "Using n-grams is part of the embedding, thus we will have to return to our scikit-learn vectorizers. There is a built in parameter for these like `ngram_range=(1, 3)`, where in this case we will generate all singe words, pairs, and triplets. This is most easily explained through an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The `token_pattern` will pick out all words (the default ignores words with one letter)\n",
    "You may include this in other pieces of code, above or below, if you want to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "sentence =  ['this sentence is a short sentence']\n",
    "\n",
    "ngrammer = CountVectorizer(lowercase=True, \n",
    "                           ngram_range=(2, 2),\n",
    "                           token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\",\n",
    "                           max_df=1.0)\n",
    "ngrammer = ngrammer.fit(sentence)\n",
    "encoded = ngrammer.transform(sentence)\n",
    "\n",
    "print(ngrammer.vocabulary_)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "If you try different values for the `ngram_range` in the code able then you should find that the values are the lower and higher bounds of the n-gram range. For example with `ngram_range=(2, 2)` we only get bi-grams. With `ngram_range=(1, 3)` we would get words, bi-grams and tri-grams.\n",
    "\n",
    "You can now check if adding n-grams helps the overall model. Below we have copy pasted the tf-idf encoding using the logistic regression model (renaming the variables).\n",
    "\n",
    "Also, the parameter `max_df` can cut out words which occur to often in your texts. For example is the word `the` occurs in 94% of all the texts, it might be actually making it harder for our classifier. You may thus want to play with this parameter. Also you will have to think about how overfitting might effect the results with a high value for the upper n-gram bound.\n",
    "\n",
    "Finally default value for `ngram_range` is `(1, 1)`, so using that should give the results you already achieved above (with `max_df=1.0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "### Test how chaning the ngram_range effects results:\n",
    "\n",
    "fancy_vect = TfidfVectorizer(lowercase=True, ngram_range=(1, 1), max_df=1.0)\n",
    "fancy_vect = fancy_vect.fit(X)\n",
    "X_train_fancy = fancy_vect.transform(X_train)\n",
    "X_test_fancy = fancy_vect.transform(X_test)\n",
    "    \n",
    "# Fitting the model:\n",
    "clf_lg = LogisticRegression()\n",
    "clf_lg.fit(X_train_fancy, y_train)\n",
    "\n",
    "# Now evaluating our model:\n",
    "predictions = clf_lg.predict(X_test_fancy)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Log. Reg. accuracy with tfidf: {}\\n'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Extra - Buliding a pipeline and configuring the Tfidf and Bow vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "We have now gone through quite a lot of options and ways to build our movie classifier. We now want the option that will classify new movies as drama or not as accurately as possible.\n",
    "\n",
    "There are roughly equal numbers of drama to non-drama movies and we don't care especially about getting one category correct at the cost of the other, so accuracy for the training set will be good enough to measure model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "But first we will take a short detour over a point we skimmed over in our word embeddings.\n",
    "\n",
    "How many words does the description of `X[20]` have? Is this consistent with the number of words in the bow embedding (`X_sparse[20]`)? You can use the `.sum()` method for sparse matricies, and `len(string.split())` function to split the string by whitespace and count the items in the resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "You should have gotten a difference of 4 actually. Hopefully by printing the description you can work out why. The answer is also in the help section at the bottom of this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {}
   },
   "source": [
    "Scikit-learn has objects called [pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which can essentially string together various parts of our model. These will be helpful in testing different models quickly. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "bow_lr = Pipeline([\n",
    "    ('bow', CountVectorizer(lowercase=True)),\n",
    "    ('lr', LogisticRegression()), ])\n",
    "bow_lr.fit(X_train, y_train)\n",
    "print(np.mean(bow_lr.predict(X_test) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "We are using almost entirely the default settings of the models and embeddings which we got from scikit-learn. To give some idea of the customizability of this we could instead change our models to something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "`text.ENGLISH_STOP_WORDS` are a list of basic words the ending will ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('wtv', TfidfVectorizer(max_df=0.3,\n",
    "                              stop_words=sklearntext.ENGLISH_STOP_WORDS,\n",
    "                              lowercase=False,\n",
    "                              ngram_range=(1, 3))),\n",
    "    ('log_reg', LogisticRegression()) ])\n",
    "pipe.fit(X_train, y_train)\n",
    "print(np.mean(pipe.predict(X_test) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Suddenly we have many different parameters for a single model, and we must use a combination of actually testing different combinations, and considering which options we think are most likely to work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Changing `random_state`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "We have quite a lot of options and outcomes of models, and seem ready to generate a lot of new data, but if you play with the `random_state` variable you will see we are at the limits of how much we can tune our model given the small size of our dataset. The last thing you want to do is investigate a model for days, and begin to see structure which really isn't there and you would get roughly similar results simply using the best we have found thus fat (the default logistic regression model with a word2vec encoding).\n",
    "\n",
    "However, you are encouraged to play with the model and try and to improve it's accuracy. In the help section at the bottom of this document I have made a function which finds the optimal regularization value, and get an accuracy of roughly 72% on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Feel free to use any combination of the tools above to create\n",
    "# a model with decent accuracy. Check either the pipelines above,\n",
    "# or even try encapsulating a model in a function so you can scan\n",
    "# over it more easily, as is done in the help section below.\n",
    "\n",
    "### Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Extra reading and acknowledgements\n",
    "\n",
    "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "http://www.davidsbatista.net/blog/2017/04/01/document_classification/\n",
    "\n",
    "https://github.com/alexandres/lexvec#pre-trained-vectors\n",
    "\n",
    "http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html\n",
    "\n",
    "https://stats.stackexchange.com/questions/267169/how-to-use-pre-trained-word2vec-model\n",
    "\n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {}
   },
   "source": [
    "# Help with exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Length of strings and bow representation\n",
    "\n",
    "You should get a srting of 31 words, and only 27 are encoded in the bow model. This is because the `CountVectorizer()` ignores all words of length 1 and there are 4 instances of 'a' in the description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Checking for a skewed data skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print('Number of dramas / number of non-dramas: {} / {}'.format(sum(y), len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Optimising the regularization in the best 'out of the box' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def model_search(c_val):\n",
    "    pipe = Pipeline([('tfidf', WordVecVectorizer(model)),\n",
    "                     ('log_reg', LogisticRegression(C=c_val, random_state=42)) ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    return np.mean(pipe.predict(X_test) == y_test)\n",
    "\n",
    "a_s = np.random.rand(100,)*8 # random float number in [0,8)\n",
    "data = []\n",
    "for a in a_s:\n",
    "    data.append([a, model_search(a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df_op = pd.DataFrame(data, columns=['C', 'Accuracy'])\n",
    "df_op.plot(x='C', y='Accuracy', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
