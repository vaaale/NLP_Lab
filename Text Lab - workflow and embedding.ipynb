{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "pycharm": {}
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "To allow easier navigation please go to\n",
        "\n",
        "[edit]$\\rightarrow$[nbextensions config]\n",
        "\n",
        "This will open a new window. In this new window select/tick\n",
        "\n",
        "* Collapsible Headings\n",
        "* Table of Contents (2)\n",
        "\n",
        "You can then simple close that window, and refresh this page.\n",
        "\n",
        "In the toolbar above you can now click the following icon to get a contents menu.\n",
        "\n",
        "\u003cimg src\u003d\"Images/ContentsButton.png\" align\u003d\u0027middle\u0027\u003e\n",
        "\n",
        "Please now run the following code to make the notebook just a little more beautiful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import HTML\n",
        "css_file_path \u003d \u0027custom.css\u0027\n",
        "styles \u003d open(css_file_path, \"r\").read()\n",
        "HTML(styles) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "pycharm": {}
      },
      "source": [
        "We now install and then import the following libraries that you will use in this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "!pip install gensim matplotlib pandas sklearn --user"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "!pip install --upgrade pandas --user"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "\n",
        "import sklearn.feature_extraction.text as sklearntext\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Import the IMDB data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We will be investigating data from the IMDB wedsite. The dataset is ready to be inported, but it can also be found [here](https://www.kaggle.com/PromptCloudHQ/imdb-data).\n",
        "\n",
        "Begin by loading the dataset into a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "data_path \u003d \u0027IMDB-Movie-Data.csv\u0027\n",
        "df_all \u003d pd.read_csv(data_path, sep\u003d\u0027,\u0027)\n",
        "df_all.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We will be trying to predict if a movie is a drama given its description.\n",
        "\n",
        "There is a lot of extra information in the dataframe, `df_all`, so we\u0027ll begin by just extracting the desctiption and the genre.\n",
        "\n",
        "We call the new dataframe df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "df \u003d df_all[[\u0027Genre\u0027, \u0027Description\u0027]].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The only standardisation of the text we will do is to make the text lower case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "df[\u0027Description\u0027] \u003d df[\u0027Description\u0027].str.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "It will be useful later on to have the length (number of words) of each movie description, so lets add that as a column to our dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003cdiv class\u003d\"alert alert-block alert-info\"\u003e\n",
        "To get the word counts we could use scikit-learn\u0027s [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), but we will introduce this tool in a later section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "df[\u0027NumWords\u0027] \u003d df[\u0027Description\u0027].apply(lambda x: len(x.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We will also need a column indicating if the genre is *drama* or *non-drama* (this will be our labels, y). We will have\n",
        "* 0 $\\rightarrow$ non-drama\n",
        "* 1 $\\rightarrow$ drama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "df[\u0027Drama\u0027] \u003d df[\u0027Genre\u0027].str.contains(\u0027Drama\u0027).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Our dataframe now has the form:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "df.sample(n\u003d10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Explorative data analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "One simple way of thinking about EDA is by asking which problems might *might* we run into with this dataset? For each possible problem we need to understand what the data looks like so we can deal with is. So lets look into\n",
        "  * Is there a wide variance in description length, or examples with no description at all?\n",
        "  * Is our data skewed (that is, do we have a strong imbalance between dramas and non-dramas)?\n",
        "  * The total number of words, and how many words occur only once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Description lenghts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Lets begin with the description lenths. We want to check the mean and std of the number of words in theinput texts, lets first plot these, and then we will use pandas to calculate the mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "df.hist(column\u003d\u0027NumWords\u0027, by\u003d\u0027Drama\u0027, layout\u003d(1,2), bins\u003dnp.linspace(0, 80, 81), figsize\u003d(20,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "df.groupby(\u0027Drama\u0027).agg([\u0027mean\u0027, \u0027median\u0027, \u0027sum\u0027, \u0027std\u0027])[\u0027NumWords\u0027]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Do these distributions look OK to work with?\n",
        "\n",
        "Does it matter that the mean isn\u0027t identical between the two categories?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Skewed data set\n",
        "\n",
        "Pandas\u0027 `.groupby` can also allow us very easily to see if we have a skewed dataset. By skewed we mean that the number of examples for one category is much less than for other other. Below we find this is not the case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "df.groupby(\u0027Drama\u0027).agg([\u0027count\u0027])[\u0027NumWords\u0027]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Finally we actually want two `np.array` which store the inputs and labels for out ML model. These can now be easily taken from our dataframe `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "X \u003d np.array(df.loc[:,\u0027Description\u0027])\n",
        "y \u003d np.array(df.loc[:,\u0027Drama\u0027])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "To be clear, we now have two numpy arrays, one, `X` with a list of movie descriptions, and one, `y`, with labes 0 and 1 for if that movie is a drama or not. Lets have a look at a \"random\" sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(f\u0027{X[42]} -\u003e {y[42]}\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Word counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "What about the total **number of words**, and the total **number of unique words**? To do this we will use scikit-learn\u0027s [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), which does a bag or words embedding. This is a bit to learn, but it will be useful later, so lets take a brief tour through the `CountVectorizer()` object, and it\u0027s output, `X_sparse`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003cdiv class\u003d\"alert alert-block alert-info\"\u003e\n",
        "This sparse matrix `X_sparse` *is* a bag of words embedding, but for now we are using it as a convenient way of counting the number of words and unique words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Lets make a `CountVectorizer`, called `count_vect`, fit it to our input, `X`, and then transform our input texts, `X`, into a bag of words embedding called `X_sparse`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "count_vect \u003d CountVectorizer(lowercase\u003dTrue)\n",
        "count_vect \u003d count_vect.fit(X)\n",
        "X_sparse \u003d count_vect.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The bag of words embedding uses a python dictionary (like a hashmap) to convert words to numbers. In python, dictionaries work in one direction, so lets also make a dictionary to convert our numbers back to words (an \u0027inverse hashmap\u0027). Note that in python accessing values in a dictionary is done like:\n",
        "\n",
        "```python\n",
        "value \u003d dictionay[key]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "vocab \u003d count_vect.vocabulary_\n",
        "vocab_inverse \u003d {}\n",
        "for word, int_embedding in vocab.items(): # In Python 2: .iteritems()\n",
        "    vocab_inverse[int_embedding] \u003d word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We now have two dictionaries for our embeddings (these are what scikit-learn us using to convert between `X` and `X_sparse`):\n",
        "\n",
        "`vocab`: $\\:\\:\\textrm{word} \\rightarrow \\textrm{number}$\n",
        "\n",
        "`vocab_inverse`: $\\:\\:\\textrm{number} \\rightarrow \\textrm{word}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "If you want to, print out either of `vocab` or `vocab_inverse` to see what these dictionaries look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(vocab)\n",
        "print(vocab_inverse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "By investigating an example we should see what is contained within `X` and `X_sparse`. Note that `X` is a list of *all* the input data, so `X[0]` should be the description of the \u0027first\u0027 movie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "X[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "`X_sparse` is a sparse matrix with all the input data encoded using bag of words. Again `X_sparse[42]` is the encoding of the \u0027first\u0027 movie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(X_sparse[42])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Finally we can check the encodings of any words. For example number 5288 should be a word appearing twice in the description:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(vocab_inverse[5288])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In movie `X[20]` they mention a geologist. Which number word is `geologist` encoded as?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "#### Your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Back to the word count problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "OK, back to the original problem, counting the number of words and unique words. Now that we have our sparse matrix `X_sparse` this should be very easy. We can just use the `.sum()` method of sparse matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "total_words \u003d X_sparse.sum()\n",
        "total_words_unique \u003d len(count_vect.vocabulary_)\n",
        "\n",
        "print(\u0027total_words: {}\u0027.format(total_words))\n",
        "print(\u0027total_words_unique: {}\u0027.format(total_words_unique))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Why do we care about the total number of words? And why did we check the number of unique words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Splitting data into test and training sets\n",
        "\n",
        "We will just use scikit-learns built in function for this. Notice that `sklearn.model_selection.train_test_split` shuffles the dataset, and then splits it. This means that `X[0]` and `X_train[0]` (or `X_test[0]`) most likely won\u0027t represent the same movie description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test \u003d train_test_split(X, y, test_size\u003d0.3, random_state\u003d42)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Creating the word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In this section we are going to create 6 matrices, which we will use as input to our machine learning models in the next section. The 6 matrices will be the following:\n",
        "\n",
        "* `X_train_bow` and  `X_test_bow`\n",
        "* `X_train_tfidf` and `X_test_tfidf`\n",
        "* `X_train_wtv` and `X_test_wtv`\n",
        "\n",
        "That is a training and a test matrix for each of the three embeddings we are using: bag of words (bow), tf-idf (tfidf) and Word2Vec (wtv)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Bag of Words (bow) and Text-frequency Inverse-document-frequency (tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We have actually already created a bow emdedding using scikit-learn\u0027s `CountVectorizer`, however we need to now make this for the train and test sets. We will also use scikit-learn\u0027s [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for the tfidf embedding.\n",
        "\n",
        "Previously we used the `.fit()` and `.transform()`. Here we will take a closer look at what they are doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003cdiv class\u003d\"alert alert-block alert-info\"\u003e\n",
        "It is also common to see the `.fit_transform()` method which does boths steps at once, although it is a little less clear what is happening in that case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "First we essentially initialise a `CountVectorizer()` object, this is the tool we use to make our embedding. This object doesn\u0027t know about our dataset yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "count_vect \u003d CountVectorizer(lowercase\u003dTrue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Then we need to \u0027fit\u0027 that vectorizer to our dataset (using the `.fit()` method). You can think of this as the vectorizer learning which set of words it need to be *able* to encode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "count_vect \u003d count_vect.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now that we have a vectorizer (called `count_vect` in this case), we can use that to transform the dataset (i.e. do the bag of words embedding). We do this with the `.transform()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "X_train_bow \u003d count_vect.transform(X_train)\n",
        "X_test_bow \u003d count_vect.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now we will go through the process again, but instead using a tf-idf vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "tfidf_vect \u003d TfidfVectorizer(lowercase\u003dTrue, max_df\u003d1.0)\n",
        "tfidf_vect \u003d tfidf_vect.fit(X)\n",
        "X_train_tfidf \u003d tfidf_vect.transform(X_train)\n",
        "X_test_tfidf \u003d tfidf_vect.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "You should print out some sample encodings from `X_train_bow` and `X_train_tfidf` to check you understand what these represent. You can also print the corresponding sample from `X_train`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003cdiv class\u003d\"alert alert-block alert-info\"\u003e\n",
        "`X_train_bow` and `X_train_tfidf` are just scikit-learn *sparse matrices*, the same as `X_sparse` was, the methods used in the word counts section should work here too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "### Your code here\n",
        "print(X_train_bow[42])\n",
        "print(X_train_tfidf[42])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Dense (Word2Vec) word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Before we do the actual word embeddings we will investigate how these embeddings work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Creating dense word embedding using Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Using [gensim](https://radimrehurek.com/gensim/) one can create their own word2vec word embeddings.\n",
        "\n",
        "We will start by **building** a simple word2vec model following [this](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/) blog post. Here we train on the senteces given in the list `sentences`. This is obviously a *very* small \u0027dataset\u0027, and thus the results just show what an embedding can look like.\n",
        "\n",
        "If you wish, you can change either `min_count` or `size` to see what they do. But be sure to change them back to `size\u003d100, min_count\u003d1` before moving on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {},
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Input dataset:\n",
        "sentences \u003d [[\u0027this\u0027, \u0027is\u0027, \u0027the\u0027, \u0027first\u0027, \u0027sentence\u0027, \u0027for\u0027, \u0027word2vec\u0027],\n",
        "             [\u0027this\u0027, \u0027is\u0027, \u0027the\u0027, \u0027second\u0027, \u0027sentence\u0027],\n",
        "             [\u0027yet\u0027, \u0027another\u0027, \u0027sentence\u0027],\n",
        "             [\u0027one\u0027, \u0027more\u0027, \u0027sentence\u0027],\n",
        "             [\u0027and\u0027, \u0027the\u0027, \u0027final\u0027, \u0027sentence\u0027]]\n",
        "\n",
        "s1 \u003d \"President Trump confirms CIA chief secretly visited North Korean leader last week\".split(\" \")\n",
        "s2 \u003d \"Barbara Bush former US first lady literacy campaigner has died\".split(\" \")\n",
        "s3 \u003d \"One person killed after US passenger jet with engine failure made emergency landing in Philadelphia\".split(\" \")\n",
        "s4 \u003d \"The prime minister personally apologized Caribbean leaders\".split(\" \")\n",
        "\n",
        "sentences \u003d [s1, s2, s3, s4]\n",
        "\n",
        "\n",
        "# Train the model:\n",
        "super_simple_model \u003d Word2Vec(sentences,  size\u003d100, min_count\u003d1)\n",
        "\n",
        "print(super_simple_model)\n",
        "# print(list(model.wv.vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "After training the model we reduce the word embedding dimension from 100 -\u003e 2 using [t-distributed Stochastic Neighbor Embedding](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) then plot the words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Transform the data via PCA to 2D so we can visually plot it.\n",
        "model_embeddings \u003d super_simple_model.wv[super_simple_model.wv.vocab]\n",
        "pca \u003d TSNE(n_components\u003d2)\n",
        "result \u003d pca.fit_transform(model_embeddings)\n",
        "\n",
        "plt.figure(1, figsize\u003d(16, 8))\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "words \u003d list(super_simple_model.wv.vocab)\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy\u003d(result[i, 0], result[i, 1]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "With so few words in our dataset, and since there is no clear meaning on the axes it\u0027s hard to assess the embedding, but the words have some strucutre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Exploring dense word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "For english there are many large and high quality embeddings that already exist (pre built embeddings also exist for many other languages).\n",
        "\n",
        "We will import a word2vec embedding created from a dataset of roughly 100 billion words from google news. The embedding covers 3 million words, but to save space, we have cut it down to 200,000 words. Each word is encoded to a 300 dimensional vector, hence the binary file imported above is aroung 250 MB.\n",
        "\n",
        "Please run the next code section. If the download takes a while just move on to the step, we won\u0027t need the file for a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "###!wget -O gW2V.bin https://www.dropbox.com/s/tu5045ajmj20ih9/googleNewsCut.bin?dl\u003d1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Lets now load the google news dataset into a gensim model object (this will take ~2 mins)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "model \u003d gensim.models.KeyedVectors.load_word2vec_format(\u0027/srv/data/gW2V.bin\u0027, binary\u003dTrue)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Gensim\u0027s `model` object has a range of useful methods for investigating the word2vec model. The rest of this section (down to **Word2Vec embedding for our IMDB data**) is now left completely open for you to play with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Lets see what an embedding looks like. You can print the whole thing if you like (just remove `[:20]`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "word \u003d \u0027house\u0027\n",
        "print(\u0027Word: {}\u0027.format(word))\n",
        "print(\u0027First 20 values of embedding:\\n{}\u0027.format(model[word][:20]))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "A classic example to demonstrate the usefulness of word2vec, is solving\n",
        "\n",
        "$X - woman \u003d king - man$\n",
        "\n",
        "for $X$. Note that you can think of this as asking the question \"what is to woman, what king is to man?\"\n",
        "\n",
        "If you want a challenge use the fact that London is the capital of England to find the capital of Norway. What other capitals does it know? Does it know capitals at all?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(model.most_similar(positive\u003d[\u0027woman\u0027, \u0027king\u0027], negative\u003d[\u0027man\u0027], topn\u003d3))\n",
        "\n",
        "print(model.most_similar(positive\u003d[\u0027France\u0027, \u0027Viking\u0027], negative\u003d[\u0027Norway\u0027], topn\u003d3))\n",
        "\n",
        "print(model.most_similar(positive\u003d[\u0027Tennis\u0027, \u0027Ronaldo\u0027], negative\u003d[\u0027Soccer\u0027], topn\u003d3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The method `.doesnt_match()` finds the word which is furtherest away from the other words in the embedding space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# .split() simply splits the sentence into a list of words\n",
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Word embeddings are built by analysing which words are used in conjunction with other words in the data set. Thus biases within the dataset will be present in the embeddings.\n",
        "\n",
        "`.similarity()` calculate how silimar two words are using the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(model.similarity(\u0027man\u0027, \u0027nurse\u0027) - model.similarity(\u0027man\u0027, \u0027doctor\u0027))\n",
        "print(model.similarity(\u0027women\u0027, \u0027nurse\u0027) - model.similarity(\u0027women\u0027, \u0027doctor\u0027))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Word2Vec embedding for our IMDB data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We will just use the google news embeddings to change our movie description to word2vec vectors. It will be useful to [wrap the embeddings into a class](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/) with fit and transform methods such that we can use them easily with scikit-learn. Although we will not need to fit this, because the embeddings are already \u0027fitted\u0027.\n",
        "\n",
        "We will use the simplest approach to encoding the description, which is to simply take the mean of the embeddings of all the words in the movie description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "class WordVecVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec \u003d word2vec\n",
        "        self.dim \u003d 300\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[w] for w in texts.split() if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis\u003d0)\n",
        "            for texts in X\n",
        "        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "wtv_vect \u003d WordVecVectorizer(model)\n",
        "X_train_wtv \u003d wtv_vect.transform(X_train)\n",
        "X_test_wtv \u003d wtv_vect.transform(X_test)\n",
        "\n",
        "print(X_train_wtv.shape) # Should be 700 (num of example) by 300 (dim. of embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Fantastic, now we have both test and training data sets with bow, tf-idf and word2vec encoding. These are stored in the matrices\n",
        "\n",
        "* `X_train_bow` and  `X_test_bow`\n",
        "* `X_train_tfidf` and `X_test_tfidf`\n",
        "* `X_train_wtv` and `X_test_wtv`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Training and predicting\n",
        "\n",
        "We will now try linear regression, SVM, a decision tree, and a random forest to flassify the movie genres for the three embeddings.\n",
        "\n",
        "The basic method is\n",
        " 1. Create a classifier object like `clf \u003d LogisticRegression()`\n",
        " 2. Then fit this object to the training set using `.fit()`\n",
        " 3. Use the model to predict the labels for the test set using `.predict()`\n",
        " 4. Elavuate the model using one of the metris from [`sklearn.metrics`](http://scikit-learn.org/stable/modules/model_evaluation.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "First lets load the models from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "OK, step 1, create the classifier object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "clf_lg \u003d LogisticRegression(random_state\u003d42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now fit this classifier. We will jsut use the tf-idf encodings, and of course we have to give the labels to it also."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "clf_lg.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "And then evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "predictions \u003d clf_lg.predict(X_test_tfidf)\n",
        "accuracy \u003d accuracy_score(y_test, predictions)\n",
        "print(\u0027LR accuracy with tfidf: {}\\n\u0027.format(accuracy))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "To understand the output, lets just look at a single example. Note that `clf_lr` is now our *fitted* linear regression model. Reading each line you can hopefully find what the `.predict()` and `.predict_proba` methods do. If not you can ask, or [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) is the documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "i \u003d 42\n",
        "print(X_train[i])\n",
        "print(clf_lg.predict(X_train_tfidf[i]))\n",
        "print(clf_lg.predict_proba(X_train_tfidf[i]))\n",
        "print(y_train[i]) # 0\u003dnon Drama, 1\u003dDrama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Lets now find the accuracy of a *support vector machine* with tf-idf encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# The default SGDClassifier implements a SVM:\n",
        "clf_svm \u003d SGDClassifier(random_state\u003d42)\n",
        "clf_svm.fit(X_train_tfidf, y_train)\n",
        "\n",
        "predictions \u003d clf_svm.predict(X_test_tfidf)\n",
        "accuracy \u003d accuracy_score(y_test, predictions)\n",
        "print(\u0027SVM accuracy with tfidf: {}\\n\u0027.format(accuracy))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Below we have named the models, as this will be used later. By setting the `random_state` variable we simply give the see to the random number generator a fixed value. You are free to play with this, or the number or trees, `n_estimators` in the random forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "clf_dt \u003d DecisionTreeClassifier(random_state\u003d42)\n",
        "\n",
        "### Your code here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "clf_rf \u003d RandomForestClassifier(n_estimators\u003d200, random_state\u003d42)\n",
        "\n",
        "### Your code here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The following simply runs over models and encoding and looks at accuracy to judge which model is the best. We have not fine tuned the models at all, so this just provides an overview of what we have done and indicates which methods are most promising."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "ecodings \u003d [X_train_bow, X_train_tfidf, X_train_wtv]\n",
        "ecodings_t \u003d [X_test_bow, X_test_tfidf, X_test_wtv]\n",
        "models \u003d [clf_lg, clf_svm, clf_rf, clf_dt]\n",
        "\n",
        "I \u003d pd.Index([\"Bow\", \"Tf-idf\", \"Word2Vec\"], name\u003d\"Encodings:\")\n",
        "C \u003d pd.Index([\"LR\", \"SVM\", \"RF\", \u0027DT\u0027], name\u003d\"Models:\")\n",
        "df \u003d pd.DataFrame(index\u003dI, columns\u003dC)\n",
        "for i, (e, e_t) in enumerate(zip(ecodings, ecodings_t)):\n",
        "    for j, m in enumerate(models):\n",
        "        m.fit(e, y_train)\n",
        "        predictions \u003d m.predict(e_t)\n",
        "        df.iloc[i, j] \u003d accuracy_score(y_test, predictions)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Adding  n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Thus far we in all of our word encodings we have only dealt with single words and have not taken into account the order of the words in the sentences. This can be done by using n-grams (sets of n words together which we treat as a single object).\n",
        "\n",
        "Although it is conceptually possible to create n-grams for the word2vec model this is not generally done and we will just investigate n-grams for the bag of words and tf-idf models.\n",
        "\n",
        "Using n-grams is part of the embedding, thus we will have to return to our scikit-learn vectorizers. There is a built in parameter for these like `ngram_range\u003d(1, 3)`, where in this case we will generate all singe words, pairs, and triplets. This is most easily explained through an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003cdiv class\u003d\"alert alert-block alert-info\"\u003e\n",
        "The `token_pattern` will pick out all words (the default ignores words with one letter)\n",
        "You may include this in other pieces of code, above or below, if you want to see the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "sentence \u003d  [\u0027this sentence is a short sentence\u0027]\n",
        "\n",
        "ngrammer \u003d CountVectorizer(lowercase\u003dTrue, \n",
        "                           ngram_range\u003d(2, 2),\n",
        "                           token_pattern\u003du\"(?u)\\\\b\\\\w+\\\\b\",\n",
        "                           max_df\u003d1.0)\n",
        "ngrammer \u003d ngrammer.fit(sentence)\n",
        "encoded \u003d ngrammer.transform(sentence)\n",
        "\n",
        "print(ngrammer.vocabulary_)\n",
        "print(encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "If you try different values for the `ngram_range` in the code able then you should find that the values are the lower and higher bounds of the n-gram range. For example with `ngram_range\u003d(2, 2)` we only get bi-grams. With `ngram_range\u003d(1, 3)` we would get words, bi-grams and tri-grams.\n",
        "\n",
        "You can now check if adding n-grams helps the overall model. Below we have copy pasted the tf-idf encoding using the logistic regression model (renaming the variables).\n",
        "\n",
        "Also, the parameter `max_df` can cut out words which occur to often in your texts. For example is the word `the` occurs in 94% of all the texts, it might be actually making it harder for our classifier. You may thus want to play with this parameter. Also you will have to think about how overfitting might effect the results with a high value for the upper n-gram bound.\n",
        "\n",
        "Finally default value for `ngram_range` is `(1, 1)`, so using that should give the results you already achieved above (with `max_df\u003d1.0`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "### Test how chaning the ngram_range effects results:\n",
        "\n",
        "fancy_vect \u003d TfidfVectorizer(lowercase\u003dTrue, ngram_range\u003d(1, 1), max_df\u003d1.0)\n",
        "fancy_vect \u003d fancy_vect.fit(X)\n",
        "X_train_fancy \u003d fancy_vect.transform(X_train)\n",
        "X_test_fancy \u003d fancy_vect.transform(X_test)\n",
        "    \n",
        "# Fitting the model:\n",
        "clf_lg \u003d LogisticRegression()\n",
        "clf_lg.fit(X_train_fancy, y_train)\n",
        "\n",
        "# Now evaluating our model:\n",
        "predictions \u003d clf_lg.predict(X_test_fancy)\n",
        "accuracy \u003d accuracy_score(y_test, predictions)\n",
        "print(\u0027Log. Reg. accuracy with tfidf: {}\\n\u0027.format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Extra - Buliding a pipeline and configuring the Tfidf and Bow vectorizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We have now gone through quite a lot of options and ways to build our movie classifier. We now want the option that will classify new movies as drama or not as accurately as possible.\n",
        "\n",
        "There are roughly equal numbers of drama to non-drama movies and we don\u0027t care especially about getting one category correct at the cost of the other, so accuracy for the training set will be good enough to measure model quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "But first we will take a short detour over a point we skimmed over in our word embeddings.\n",
        "\n",
        "How many words does the description of `X[20]` have? Is this consistent with the number of words in the bow embedding (`X_sparse[20]`)? You can use the `.sum()` method for sparse matricies, and `len(string.split())` function to split the string by whitespace and count the items in the resulting list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "### Your code here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "You should have gotten a difference of 4 actually. Hopefully by printing the description you can work out why. The answer is also in the help section at the bottom of this document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "source": [
        "Scikit-learn has objects called [pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which can essentially string together various parts of our model. These will be helpful in testing different models quickly. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "bow_lr \u003d Pipeline([\n",
        "    (\u0027bow\u0027, CountVectorizer(lowercase\u003dTrue)),\n",
        "    (\u0027lr\u0027, LogisticRegression()), ])\n",
        "bow_lr.fit(X_train, y_train)\n",
        "print(np.mean(bow_lr.predict(X_test) \u003d\u003d y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We are using almost entirely the default settings of the models and embeddings which we got from scikit-learn. To give some idea of the customizability of this we could instead change our models to something like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003cdiv class\u003d\"alert alert-block alert-info\"\u003e\n",
        "`text.ENGLISH_STOP_WORDS` are a list of basic words the ending will ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "pipe \u003d Pipeline([\n",
        "    (\u0027wtv\u0027, TfidfVectorizer(max_df\u003d0.3,\n",
        "                              stop_words\u003dsklearntext.ENGLISH_STOP_WORDS,\n",
        "                              lowercase\u003dFalse,\n",
        "                              ngram_range\u003d(1, 3))),\n",
        "    (\u0027log_reg\u0027, LogisticRegression()) ])\n",
        "pipe.fit(X_train, y_train)\n",
        "print(np.mean(pipe.predict(X_test) \u003d\u003d y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Suddenly we have many different parameters for a single model, and we must use a combination of actually testing different combinations, and considering which options we think are most likely to work well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Changing `random_state`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We have quite a lot of options and outcomes of models, and seem ready to generate a lot of new data, but if you play with the `random_state` variable you will see we are at the limits of how much we can tune our model given the small size of our dataset. The last thing you want to do is investigate a model for days, and begin to see structure which really isn\u0027t there and you would get roughly similar results simply using the best we have found thus fat (the default logistic regression model with a word2vec encoding).\n",
        "\n",
        "However, you are encouraged to play with the model and try and to improve it\u0027s accuracy. In the help section at the bottom of this document I have made a function which finds the optimal regularization value, and get an accuracy of roughly 72% on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Feel free to use any combination of the tools above to create\n",
        "# a model with decent accuracy. Check either the pipelines above,\n",
        "# or even try encapsulating a model in a function so you can scan\n",
        "# over it more easily, as is done in the help section below.\n",
        "\n",
        "### Your code here\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Extra reading and acknowledgements\n",
        "\n",
        "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "\n",
        "http://www.davidsbatista.net/blog/2017/04/01/document_classification/\n",
        "\n",
        "https://github.com/alexandres/lexvec#pre-trained-vectors\n",
        "\n",
        "http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
        "\n",
        "http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html\n",
        "\n",
        "https://stats.stackexchange.com/questions/267169/how-to-use-pre-trained-word2vec-model\n",
        "\n",
        "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "source": [
        "# Help with exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Length of strings and bow representation\n",
        "\n",
        "You should get a srting of 31 words, and only 27 are encoded in the bow model. This is because the `CountVectorizer()` ignores all words of length 1 and there are 4 instances of \u0027a\u0027 in the description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Checking for a skewed data skewed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(\u0027Number of dramas / number of non-dramas: {} / {}\u0027.format(sum(y), len(y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Optimising the regularization in the best \u0027out of the box\u0027 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def model_search(c_val):\n",
        "    pipe \u003d Pipeline([(\u0027tfidf\u0027, WordVecVectorizer(model)),\n",
        "                     (\u0027log_reg\u0027, LogisticRegression(C\u003dc_val, random_state\u003d42)) ])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    return np.mean(pipe.predict(X_test) \u003d\u003d y_test)\n",
        "\n",
        "a_s \u003d np.random.rand(100,)*8 # random float number in [0,8)\n",
        "data \u003d []\n",
        "for a in a_s:\n",
        "    data.append([a, model_search(a)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "df_op \u003d pd.DataFrame(data, columns\u003d[\u0027C\u0027, \u0027Accuracy\u0027])\n",
        "df_op.plot(x\u003d\u0027C\u0027, y\u003d\u0027Accuracy\u0027, kind\u003d\u0027scatter\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}